# -*- coding: utf-8 -*-
"""Untitled2.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/15JIer8XdGETDmgfMTr8B3uOx7noHjxZQ
"""

import numpy as np

# Data preparation
vocab = ['I', 'love', 'deep', 'learning']
word_to_idx = {w: i for i, w in enumerate(vocab)}
idx_to_word = {i: w for w, i in word_to_idx.items()}

# One-hot encode the words
def one_hot(index, size):
    vec = np.zeros(size)
    vec[index] = 1
    return vec

# Input/Target
X_words = ['I', 'love', 'deep']
Y_word = 'learning'

vocab_size = len(vocab)
hidden_size = 10  # size of hidden state

# Initialize weights
Wxh = np.random.randn(hidden_size, vocab_size) * 0.01  # input to hidden
Whh = np.random.randn(hidden_size, hidden_size) * 0.01  # hidden to hidden
Why = np.random.randn(vocab_size, hidden_size) * 0.01  # hidden to output
bh = np.zeros((hidden_size, 1))  # hidden bias
by = np.zeros((vocab_size, 1))  # output bias

# Learning rate
lr = 0.1

# Training loop
for epoch in range(100):
    inputs = [one_hot(word_to_idx[w], vocab_size).reshape(-1, 1) for w in X_words]
    target = word_to_idx[Y_word]

    # FORWARD PASS
    hs = {}
    hs[-1] = np.zeros((hidden_size, 1))  # initial hidden state

    for t in range(len(inputs)):
        hs[t] = np.tanh(np.dot(Wxh, inputs[t]) + np.dot(Whh, hs[t-1]) + bh)

    y_pred = np.dot(Why, hs[len(inputs)-1]) + by
    probs = np.exp(y_pred) / np.sum(np.exp(y_pred))  # softmax

    # Compute loss (cross-entropy)
    loss = -np.log(probs[target][0])

    # BACKWARD PASS
    dWhy = np.zeros_like(Why)
    dby = np.zeros_like(by)
    dWxh = np.zeros_like(Wxh)
    dWhh = np.zeros_like(Whh)
    dbh = np.zeros_like(bh)
    dh_next = np.zeros_like(hs[0])

    # Output gradient
    dy = probs
    dy[target] -= 1  # softmax gradient

    # Gradients for Why and by
    dWhy += np.dot(dy, hs[len(inputs)-1].T)
    dby += dy

    # Backprop through time
    for t in reversed(range(len(inputs))):
        dh = np.dot(Why.T, dy) + dh_next
        dtanh = (1 - hs[t] ** 2) * dh
        dbh += dtanh
        dWxh += np.dot(dtanh, inputs[t].T)
        dWhh += np.dot(dtanh, hs[t-1].T)
        dh_next = np.dot(Whh.T, dtanh)

    # Clip gradients to prevent exploding
    for dparam in [dWxh, dWhh, dWhy, dbh, dby]:
        np.clip(dparam, -5, 5, out=dparam)

    # Update weights
    Wxh -= lr * dWxh
    Whh -= lr * dWhh
    Why -= lr * dWhy
    bh -= lr * dbh
    by -= lr * dby

    if epoch % 10 == 0:
        pred_idx = np.argmax(probs)
        pred_word = idx_to_word[pred_idx]
        print(f'Epoch {epoch}, Loss: {loss:.4f}, Prediction: {pred_word}')